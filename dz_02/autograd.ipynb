{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["MRuSrP7JQ00i","1b95Z8u7Q3OL"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# [Pytorch autograd](https://pytorch.org/docs/stable/autograd.html)"],"metadata":{"id":"KlS1ciOPBg7g"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"yVaGmM0JHWkZ","executionInfo":{"status":"ok","timestamp":1670677118431,"user_tz":-180,"elapsed":244,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["class Value:\n","    \"\"\" stores a single scalar value and its gradient \"\"\"\n","\n","    def __init__(self, data, _children=(), _op=''):\n","        self.data = data\n","        self.grad = 0\n","        # internal variables used for autograd graph construction\n","        self._backward = lambda: None # function \n","        self._prev = set(_children) # set of Value objects\n","        self._op = _op # the op that produced this node, string ('+', '-', ....)\n","\n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data)\n","\n","        def _backward():\n","            # Calculating the derivative of the sum\n","            self.grad += out.grad \n","            other.grad += out.grad\n","        out._backward = _backward\n","        \n","        # Add children to resulting expression\n","        out._prev.add(other)\n","        out._prev.add(self)\n","\n","        return out\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data * other.data)\n","\n","        def _backward():\n","            # Calculating the derivative of the product\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","\n","        # Add children to resulting expression\n","        out._prev.add(other)\n","        out._prev.add(self)\n","\n","        return out\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n","        out = Value(self.data ** other)\n","\n","        def _backward():\n","            # Calculating the derivative of a power func\n","            self.grad += (other * self.data**(other-1)) * out.grad\n","        out._backward = _backward\n","        \n","        # Add children to resulting expression\n","        out._prev.add(self)\n","\n","        return out\n","\n","    def relu(self):\n","        out = Value(self.data) if self.data > 0 else Value(0)\n","\n","        def _backward():\n","            # Calculating the derivative of the ReLU\n","            self.grad += out.grad if out.data > 0 else 0\n","        out._backward = _backward\n","\n","        # Add children to resulting expression\n","        out._prev.add(self)\n","\n","        return out\n","\n","    def backward(self):\n","\n","        # topological order all of the children in the graph\n","        topo = []\n","        visited = set()\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        build_topo(self)\n","\n","        # go one variable at a time and apply the chain rule to get its gradient\n","        self.grad = 1\n","        for v in reversed(topo):\n","            v._backward()\n","\n","    def __neg__(self): # -self\n","        return self * -1\n","\n","    def __radd__(self, other): # other + self\n","        return self + other\n","\n","    def __sub__(self, other): # self - other\n","        return self + (-other)\n","\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","\n","    def __rmul__(self, other): # other * self\n","        return self * other\n","\n","    def __truediv__(self, other): # self / other\n","        return self * other**-1\n","\n","    def __rtruediv__(self, other): # other / self\n","        return other * self**-1\n","\n","    def __repr__(self):\n","        return f\"Value(data={self.data}, grad={self.grad})\""],"metadata":{"id":"chDdD9oSUlUJ","executionInfo":{"status":"ok","timestamp":1670677118431,"user_tz":-180,"elapsed":5,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def test_sanity_check():\n","\n","    x = Value(-4.0)\n","    z = 2 * x + 2 + x\n","  \n","    q = z.relu() + z * x\n","    h = (z * z).relu()\n","    y = h + q + q * x\n","    y.backward()\n","    xmg, ymg = x, y\n","\n","    x = torch.Tensor([-4.0]).double()\n","    x.requires_grad = True\n","    z = 2 * x + 2 + x\n","    q = z.relu() + z * x\n","    h = (z * z).relu()\n","    y = h + q + q * x\n","    y.backward()\n","    xpt, ypt = x, y\n","\n","    \n","    # forward pass went well\n","    assert ymg.data == ypt.data.item()\n","    # backward pass went well\n","    print(xmg, xpt, xpt.grad)\n","    assert xmg.grad == xpt.grad.item()\n","\n","\n","def test_more_ops():\n","\n","    a = Value(-4.0)\n","    b = Value(2.0)\n","    c = a + b\n","    d = a * b + b**3\n","    c += c + 1\n","    c += 1 + c + (-a)\n","    d += d * 2 + (b + a).relu()\n","    d += 3 * d + (b - a).relu()\n","    e = c - d\n","    f = e**2\n","    g = f / 2.0\n","    g += 10.0 / f\n","    g.backward()\n","    amg, bmg, gmg = a, b, g\n","\n","    a = torch.Tensor([-4.0]).double()\n","    b = torch.Tensor([2.0]).double()\n","    a.requires_grad = True\n","    b.requires_grad = True\n","    c = a + b\n","    d = a * b + b**3\n","    c = c + c + 1\n","    c = c + 1 + c + (-a)\n","    d = d + d * 2 + (b + a).relu()\n","    d = d + 3 * d + (b - a).relu()\n","    e = c - d\n","    f = e**2\n","    g = f / 2.0\n","    g = g + 10.0 / f\n","    g.backward()\n","    apt, bpt, gpt = a, b, g\n","\n","    tol = 1e-6\n","    # forward pass went well\n","    assert abs(gmg.data - gpt.data.item()) < tol\n","    # backward pass went well\n","    assert abs(amg.grad - apt.grad.item()) < tol\n","    assert abs(bmg.grad - bpt.grad.item()) < tol"],"metadata":{"id":"vY7OzWjuUiaa","executionInfo":{"status":"ok","timestamp":1670677118431,"user_tz":-180,"elapsed":5,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["a = Value(-4.0)\n","b = Value(2.0)\n","d = Value(3.0)"],"metadata":{"id":"1LgTiYeZ-WGk","executionInfo":{"status":"ok","timestamp":1670677118432,"user_tz":-180,"elapsed":6,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["c = a + b\n","e = c * d\n","e.backward()"],"metadata":{"id":"Y0svSAs2h0Ap","executionInfo":{"status":"ok","timestamp":1670677118432,"user_tz":-180,"elapsed":5,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["test_sanity_check()"],"metadata":{"id":"w9n8DN6RYkrx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670677118432,"user_tz":-180,"elapsed":5,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}},"outputId":"d37d2ab5-c01a-4436-ea85-1deb02b49502"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Value(data=-4.0, grad=46.0) tensor([-4.], dtype=torch.float64, requires_grad=True) tensor([46.], dtype=torch.float64)\n"]}]},{"cell_type":"code","source":["test_more_ops()"],"metadata":{"id":"1T198QDQYh_q","executionInfo":{"status":"ok","timestamp":1670677118432,"user_tz":-180,"elapsed":4,"user":{"displayName":"Leodroid","userId":"10836640892735204266"}}},"execution_count":52,"outputs":[]}]}